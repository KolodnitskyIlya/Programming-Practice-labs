{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e7c8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pygame\n",
      "  Downloading pygame-2.6.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Downloading pygame-2.6.1-cp39-cp39-macosx_11_0_arm64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pygame\n",
      "Successfully installed pygame-2.6.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /Users/macbook/Library/Python/3.9/lib/python/site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/macbook/Library/Python/3.9/lib/python/site-packages (from gym) (3.1.1)\n",
      "Requirement already satisfied: importlib_metadata>=4.8.0 in /Users/macbook/Library/Python/3.9/lib/python/site-packages (from gym) (8.2.0)\n",
      "Collecting gym_notices>=0.0.4 (from gym)\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/macbook/Library/Python/3.9/lib/python/site-packages (from importlib_metadata>=4.8.0->gym) (3.20.0)\n",
      "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827727 sha256=6bea481b49b1d6ef62ea94dbc7eae942714635baab98acd8ba0fda3bfa499956\n",
      "  Stored in directory: /Users/macbook/Library/Caches/pip/wheels/af/2b/30/5e78b8b9599f2a2286a582b8da80594f654bf0e18d825a4405\n",
      "Successfully built gym\n",
      "Installing collected packages: gym_notices, gym\n",
      "Successfully installed gym-0.26.2 gym_notices-0.0.8\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pygame\n",
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ab5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.9.6)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Starting training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x11 and 9x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 408\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    406\u001b[0m         env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 408\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 391\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 391\u001b[0m     rewards_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m    394\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(rewards_history)\n",
      "Cell \u001b[0;32mIn[1], line 357\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(env, agent, episodes, batch_size, render_every)\u001b[0m\n\u001b[1;32m    354\u001b[0m             rewards_history\u001b[38;5;241m.\u001b[39mappend(total_reward)\n\u001b[1;32m    355\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Epsilon: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;241m.\u001b[39mepsilon\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 357\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rewards_history\n",
      "Cell \u001b[0;32mIn[1], line 314\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    311\u001b[0m next_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39marray([t[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m minibatch]))\n\u001b[1;32m    312\u001b[0m dones \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(np\u001b[38;5;241m.\u001b[39marray([t[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m minibatch]))\n\u001b[0;32m--> 314\u001b[0m current_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    315\u001b[0m next_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(next_states)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    316\u001b[0m target \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m next_q\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 276\u001b[0m, in \u001b[0;36mDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 276\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    277\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x11 and 9x64)"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TankEnv(gym.Env):\n",
    "    def __init__(self, render_mode=None):\n",
    "        super(TankEnv, self).__init__()\n",
    "        \n",
    "        self.map_width = 800\n",
    "        self.map_height = 600\n",
    "        self.tank_size = 20\n",
    "        self.target_size = 15\n",
    "        self.render_mode = render_mode\n",
    "        \n",
    "        self.max_steps = 500\n",
    "        self.current_step = 0\n",
    "        \n",
    "        if self.render_mode == 'human':\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((self.map_width, self.map_height))\n",
    "            pygame.display.set_caption(\"Tank Reinforcement Learning\")\n",
    "            self.clock = pygame.time.Clock()\n",
    "        \n",
    "        self.action_space = spaces.Discrete(5)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, -180, 0, -180, 0, 0, 0, 0], dtype=np.float32),\n",
    "            high=np.array([self.map_width, self.map_height, 180, \n",
    "                          math.sqrt(self.map_width**2 + self.map_height**2), \n",
    "                          180, self.map_width, self.map_height, self.map_width, self.map_height], \n",
    "                         dtype=np.float32)\n",
    "        )\n",
    "        \n",
    "        self.obstacles = [\n",
    "            pygame.Rect(100, 100, 50, 200),\n",
    "            pygame.Rect(300, 400, 200, 50),\n",
    "            pygame.Rect(600, 200, 50, 300),\n",
    "            pygame.Rect(200, 300, 150, 50)\n",
    "        ]\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.tank_pos = np.array([\n",
    "            random.randint(self.tank_size, self.map_width - self.tank_size),\n",
    "            random.randint(self.tank_size, self.map_height - self.tank_size)\n",
    "        ])\n",
    "        self.tank_angle = random.randint(0, 359)\n",
    "        \n",
    "        while True:\n",
    "            self.target_pos = np.array([\n",
    "                random.randint(self.target_size, self.map_width - self.target_size),\n",
    "                random.randint(self.target_size, self.map_height - self.target_size)\n",
    "            ])\n",
    "            target_rect = pygame.Rect(\n",
    "                self.target_pos[0] - self.target_size//2, \n",
    "                self.target_pos[1] - self.target_size//2,\n",
    "                self.target_size, self.target_size\n",
    "            )\n",
    "            \n",
    "            collision = False\n",
    "            for obstacle in self.obstacles:\n",
    "                if target_rect.colliderect(obstacle):\n",
    "                    collision = True\n",
    "                    break\n",
    "            \n",
    "            if not collision and np.linalg.norm(self.tank_pos - self.target_pos) > 200:\n",
    "                break\n",
    "        \n",
    "        self.current_step = 0\n",
    "        self.bullet_pos = None\n",
    "        self.bullet_speed = 10\n",
    "        self.bullet_direction = 0\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        distance_to_target = np.linalg.norm(self.tank_pos - self.target_pos)\n",
    "        \n",
    "        dx = self.target_pos[0] - self.tank_pos[0]\n",
    "        dy = self.target_pos[1] - self.tank_pos[1]\n",
    "        angle_to_target = math.degrees(math.atan2(dy, dx)) - self.tank_angle\n",
    "        angle_to_target = (angle_to_target + 180) % 360 - 180\n",
    "        \n",
    "        obstacle_distances = []\n",
    "        for angle_offset in [0, 180, 90, -90]:\n",
    "            angle = (self.tank_angle + angle_offset) % 360\n",
    "            obstacle_distances.append(self._get_distance_to_obstacle(angle))\n",
    "        \n",
    "        bullet_x = self.bullet_pos[0] if self.bullet_pos is not None else 0\n",
    "        bullet_y = self.bullet_pos[1] if self.bullet_pos is not None else 0\n",
    "        \n",
    "        return np.array([\n",
    "            self.tank_pos[0], self.tank_pos[1], self.tank_angle,\n",
    "            distance_to_target, angle_to_target,\n",
    "            *obstacle_distances,\n",
    "            bullet_x, bullet_y\n",
    "        ], dtype=np.float32)\n",
    "    \n",
    "    def _get_distance_to_obstacle(self, angle):\n",
    "        step = 5\n",
    "        distance = 0\n",
    "        rad_angle = math.radians(angle)\n",
    "        \n",
    "        while distance < 300:\n",
    "            distance += step\n",
    "            x = self.tank_pos[0] + distance * math.cos(rad_angle)\n",
    "            y = self.tank_pos[1] + distance * math.sin(rad_angle)\n",
    "            \n",
    "            if x < 0 or x >= self.map_width or y < 0 or y >= self.map_height:\n",
    "                return distance\n",
    "            \n",
    "            point_rect = pygame.Rect(x-2, y-2, 4, 4)\n",
    "            for obstacle in self.obstacles:\n",
    "                if point_rect.colliderect(obstacle):\n",
    "                    return distance\n",
    "        \n",
    "        return distance\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = -1\n",
    "        done = False\n",
    "        info = {}\n",
    "        \n",
    "        if action == 0:\n",
    "            self._move_tank(5)\n",
    "        elif action == 1:\n",
    "            self._move_tank(-3)\n",
    "        elif action == 2:\n",
    "            self.tank_angle = (self.tank_angle - 5) % 360\n",
    "        elif action == 3:\n",
    "            self.tank_angle = (self.tank_angle + 5) % 360\n",
    "        elif action == 4:\n",
    "            if self.bullet_pos is None:\n",
    "                self.bullet_pos = np.array(self.tank_pos)\n",
    "                self.bullet_direction = self.tank_angle\n",
    "        \n",
    "        if self.bullet_pos is not None:\n",
    "            rad_angle = math.radians(self.bullet_direction)\n",
    "            self.bullet_pos[0] += self.bullet_speed * math.cos(rad_angle)\n",
    "            self.bullet_pos[1] += self.bullet_speed * math.sin(rad_angle)\n",
    "            \n",
    "            if (self.bullet_pos[0] < 0 or self.bullet_pos[0] >= self.map_width or\n",
    "                self.bullet_pos[1] < 0 or self.bullet_pos[1] >= self.map_height):\n",
    "                self.bullet_pos = None\n",
    "            \n",
    "            if self.bullet_pos is not None:\n",
    "                bullet_rect = pygame.Rect(self.bullet_pos[0] - 2, self.bullet_pos[1] - 2, 4, 4)\n",
    "                target_rect = pygame.Rect(\n",
    "                    self.target_pos[0] - self.target_size//2, \n",
    "                    self.target_pos[1] - self.target_size//2,\n",
    "                    self.target_size, self.target_size\n",
    "                )\n",
    "                \n",
    "                if bullet_rect.colliderect(target_rect):\n",
    "                    reward += 100\n",
    "                    done = True\n",
    "                    info['result'] = 'target_hit'\n",
    "                    self.bullet_pos = None\n",
    "        \n",
    "        tank_rect = pygame.Rect(\n",
    "            self.tank_pos[0] - self.tank_size//2, \n",
    "            self.tank_pos[1] - self.tank_size//2,\n",
    "            self.tank_size, self.tank_size\n",
    "        )\n",
    "        \n",
    "        for obstacle in self.obstacles:\n",
    "            if tank_rect.colliderect(obstacle):\n",
    "                reward -= 10\n",
    "                done = True\n",
    "                info['result'] = 'obstacle_hit'\n",
    "                break\n",
    "        \n",
    "        prev_distance = np.linalg.norm(self.tank_pos - self.target_pos)\n",
    "        new_distance = np.linalg.norm(self.tank_pos - self.target_pos)\n",
    "        \n",
    "        if new_distance < prev_distance:\n",
    "            reward += 10 * (prev_distance - new_distance) / prev_distance\n",
    "        else:\n",
    "            reward -= 5 * (new_distance - prev_distance) / prev_distance\n",
    "        \n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_steps:\n",
    "            done = True\n",
    "            info['result'] = 'timeout'\n",
    "        \n",
    "        if self.render_mode == 'human':\n",
    "            self.render()\n",
    "        \n",
    "        return self._get_state(), reward, done, info\n",
    "    \n",
    "    def _move_tank(self, distance):\n",
    "        rad_angle = math.radians(self.tank_angle)\n",
    "        new_pos = np.array([\n",
    "            self.tank_pos[0] + distance * math.cos(rad_angle),\n",
    "            self.tank_pos[1] + distance * math.sin(rad_angle)\n",
    "        ])\n",
    "        \n",
    "        if (self.tank_size <= new_pos[0] <= self.map_width - self.tank_size and\n",
    "            self.tank_size <= new_pos[1] <= self.map_height - self.tank_size):\n",
    "            \n",
    "            tank_rect = pygame.Rect(\n",
    "                new_pos[0] - self.tank_size//2, \n",
    "                new_pos[1] - self.tank_size//2,\n",
    "                self.tank_size, self.tank_size\n",
    "            )\n",
    "            \n",
    "            collision = False\n",
    "            for obstacle in self.obstacles:\n",
    "                if tank_rect.colliderect(obstacle):\n",
    "                    collision = True\n",
    "                    break\n",
    "            \n",
    "            if not collision:\n",
    "                self.tank_pos = new_pos\n",
    "    \n",
    "    def render(self):\n",
    "        if self.render_mode != 'human':\n",
    "            return\n",
    "        \n",
    "        self.screen.fill((255, 255, 255))\n",
    "        \n",
    "        for obstacle in self.obstacles:\n",
    "            pygame.draw.rect(self.screen, (100, 100, 100), obstacle)\n",
    "        \n",
    "        pygame.draw.circle(\n",
    "            self.screen, (255, 0, 0),\n",
    "            (int(self.target_pos[0]), int(self.target_pos[1])),\n",
    "            self.target_size\n",
    "        )\n",
    "        \n",
    "        tank_center = (int(self.tank_pos[0]), int(self.tank_pos[1]))\n",
    "        tank_rect = pygame.Rect(\n",
    "            tank_center[0] - self.tank_size//2,\n",
    "            tank_center[1] - self.tank_size//2,\n",
    "            self.tank_size, self.tank_size\n",
    "        )\n",
    "        pygame.draw.rect(self.screen, (0, 0, 255), tank_rect)\n",
    "        \n",
    "        end_pos = (\n",
    "            tank_center[0] + self.tank_size * math.cos(math.radians(self.tank_angle)),\n",
    "            tank_center[1] + self.tank_size * math.sin(math.radians(self.tank_angle))\n",
    "        )\n",
    "        pygame.draw.line(self.screen, (0, 255, 0), tank_center, end_pos, 3)\n",
    "        \n",
    "        if self.bullet_pos is not None:\n",
    "            pygame.draw.circle(\n",
    "                self.screen, (255, 165, 0),\n",
    "                (int(self.bullet_pos[0]), int(self.bullet_pos[1])),\n",
    "                3\n",
    "            )\n",
    "        \n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(30)\n",
    "    \n",
    "    def close(self):\n",
    "        if self.render_mode == 'human':\n",
    "            pygame.quit()\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = DQN(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = torch.FloatTensor(state)\n",
    "        act_values = self.model(state)\n",
    "        return torch.argmax(act_values).item()\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states = torch.FloatTensor(np.array([t[0] for t in minibatch]))\n",
    "        actions = torch.LongTensor(np.array([t[1] for t in minibatch]))\n",
    "        rewards = torch.FloatTensor(np.array([t[2] for t in minibatch]))\n",
    "        next_states = torch.FloatTensor(np.array([t[3] for t in minibatch]))\n",
    "        dones = torch.FloatTensor(np.array([t[4] for t in minibatch]))\n",
    "        \n",
    "        current_q = self.model(states).gather(1, actions.unsqueeze(1))\n",
    "        next_q = self.model(next_states).max(1)[0].detach()\n",
    "        target = rewards + (1 - dones) * self.gamma * next_q\n",
    "        \n",
    "        loss = F.mse_loss(current_q.squeeze(), target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def save(self, filename):\n",
    "        torch.save(self.model.state_dict(), filename)\n",
    "    \n",
    "    def load(self, filename):\n",
    "        self.model.load_state_dict(torch.load(filename))\n",
    "        self.model.eval()\n",
    "\n",
    "def train_agent(env, agent, episodes=1000, batch_size=32, render_every=100):\n",
    "    rewards_history = []\n",
    "    \n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        render = (e % render_every == 0) and (env.render_mode == 'human')\n",
    "        \n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "            \n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                rewards_history.append(total_reward)\n",
    "                print(f\"Episode: {e+1}/{episodes}, Total reward: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n",
    "                \n",
    "            agent.replay(batch_size)\n",
    "    \n",
    "    return rewards_history\n",
    "\n",
    "def test_agent(env, agent, episodes=5):\n",
    "    total_rewards = []\n",
    "    \n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = agent.act(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                print(f\"Test Episode {e+1}, Total reward: {total_reward}\")\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "    \n",
    "    return total_rewards\n",
    "\n",
    "def main():\n",
    "    env = TankEnv(render_mode='human')\n",
    "    \n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    \n",
    "    try:\n",
    "        print(\"Starting training...\")\n",
    "        rewards_history = train_agent(env, agent, episodes=500, render_every=50)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(rewards_history)\n",
    "        plt.title('Rewards per Episode During Training')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Total Reward')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Testing trained agent...\")\n",
    "        test_rewards = test_agent(env, agent, episodes=5)\n",
    "        print(f\"Average test reward: {np.mean(test_rewards):.2f}\")\n",
    "        \n",
    "    finally:\n",
    "        env.close()\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
